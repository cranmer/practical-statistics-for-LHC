\subsection{Effective Model Narrative}


In the simulation narrative the model of discriminating variable distributions $f(x|\vec\alpha)$ is derived from discrete samples of simulated events $\{x_1,\dots,x_N\}$.   We discussed above how one can use histograms or kernel estimation to approximate the underlying distribution and interpolation strategies to incorporate systematic effects.  Another approach is to assume some parametric form for the distribution to serve as an effective model.  For example, in the $H\to\gamma\gamma$ analysis shown in Fig.~\ref{fig:H2photons} a simple exponential distribution was used to model the background.  The state-of-the-art theoretical predictions for the continuum $\gamma\gamma$ background process do not predict exactly an exponentially falling distribution, and the analysis must (and does) incorporate the systematic associated to the effective model.  Similarly, it is common to use a polynomial in some limited sideband region to estimate backgrounds under a peak.  These effective models can range from very ad hoc~\footnote{For instance, the modeling of $H\to ZZ^{(*)}\to 4l$  described in \cite{Aad:2009wy} (see Eq. 2 of the corresponding section)  } to more motivated.  For instance, one might use knowledge of kinematics and phase space and/or detector resolution to construct an effective model that captures the relevant physics.  The advantage of a well motivated effective model is that few nuisance parameters may describe well the relevant family of probability densities, which is the challenge for generic (and relatively unsophisticated) interpolation strategies usually employed in the simulation narrative.


\subsection{The Matrix Element Method}
Ideally, one would not use a single discriminating variable to distinguish the process of interest from the other background processes, but instead would use as much discriminating power as possible.  This implies forming a probability model over a multi-dimensional discriminating variable (ie. a multivariate analysis technique).  In principle, both the histogram-based and kernel-based approach generalize to distributions of multi-dimensional discriminating variables; however, in practice, they are limited to only a few dimensions. In the case of histograms this is particularly severe unless one employs clever binning choices, while in the kernel-based approach one can model up to about 5-dimensional distributions with reasonable Monte Carlo sample sizes.   In practice, one often uses multivariate algorithms like Neural Networks or boosted decision trees\footnote{A useful toolkit for high-energy physics is TMVA, which is packaged with ROOT~\cite{tmva}.} to map the multiple variables into a single discriminating variable.  Often these multivariate techniques are seen as somewhat of a black-box.  If we restrict ourselves to discriminating variables associated with the kinematics of final state particles (as opposed to the more detailed signature of particles in the detector), then we can often approximate he detailed simulation of the detector with a parametrized detector response.  If we denote the kinematic configuration of all the final state particles in the Lorentz invariant phase space as $\Phi$, the initial state as $i$,  the matrix element (potentially averaged over unmeasured spin configurations) as $\mathcal{M}(i,\Phi)$, and the probability due to parton density functions for the initial state $i$ going into the hard scattering  as $f(i)$, then we can write that the distribution of the, possibly multi-dimensional, discriminating variable $x$ as
\begin{equation}
f(x) \propto \int d\Phi \, f(i) |\mathcal{M}(i,\Phi)|^2 \, W(x | \Phi) \;,
\end{equation}
where $W(x|\Phi)$ is referred to as the transfer function of $x$ given the final state configuration $\Phi$.  It is natural to think of $W(x|\Phi)$ as a conditional distribution, but here I let $W$ encode the efficiency and acceptance so that we have
\begin{equation}
\frac{\sigma_{\rm eff.}}{\sigma} = \frac{\int dx \int d\Phi \, |\mathcal{M}(i,\Phi)|^2 \, W(x | \Phi) }{\int d\Phi \, |\mathcal{M}(i,\Phi)|^2 }\;.
\end{equation}
Otherwise, the equation above looks like another application one Bayes's theorem where $W(x|\Phi)$ plays the role of the pdf/likelihood function and $\mathcal{M}(i,\Phi)$ plays the role of the prior over the $\Phi$.  It is worth pointing out that this is a frequentist use of Bayes's theorem since $d\Phi$ is the Lorentz invariant phase space which explicitly has a measure associated with it.


\subsection{Event-by-event resolution, conditional modeling, and Punzi factors}


In some cases one would like to provide a distribution for the discriminating variable $x$ based conditional on some other observable in the event $y$: $f(x|\vec\alpha,y)$.  For instance, one might want to say that the energy resolution for electrons depends on the energy itself through a well-known calorimeter resolution parametrization like $\sigma(E)/E = A/\sqrt{E}\oplus B$.  These types of conditional distributions can be built in \roofit.  A subtle point studied by Punzi is that if $f(y|\vec\alpha)$ depends on $\vec\alpha$ the inference on $\vec\alpha$ can be biased~\cite{Punzi:2004wh}.  In particular, if one is trying to estimate the amount of signal in a sample and the distribution of $y$ for the signal is different than for the background, the estimate of the signal fraction will be biased.  This can be remedied by including terms related to $f(y|\vec\alpha)$, colloquially called `Punzi Factors'.  Importantly, this means one cannot build conditional models like this without knowing or assuming something about $f(y|\vec\alpha)$.
