How does this generalize for our most general model in Eq.~\ref{Eq:ftot} with many free parameters?  First one must still define the null and the alternate hypotheses.  Typically is done by saying some parameters -- the parameters of interest $\vec\alpha_{\rm poi}$ --  take on specific values takes on a particular value for the signal-plus-background hypothesis and a different value for the background-only hypothesis.  For instance, the signal production cross-section might be singled out as the \textit{parameter of interest} and it would take on the value of zero for the background-only and some reference value for the signal-plus-background.  The remainder of the parameters are called the \textit{nuisance parameters} $\vec\alpha_{\rm nuis}$.  Unfortunately, there is no equivalent to the Neyman-Pearson lemma for models with several free parameters -- so called, composite models.  Nevertheless, there is a natural generalization based on the profile likelihood ratio.


Remembering that the test statistic $T$ is a real-valued function of the data, then any particular probability model $\f_{\rm tot}(\data|\vec\alpha)$ implies a distribution for the test statistic $f(T|\vec\alpha)$.  Note, the distribution for the test statistic depends on the value of $\vec\alpha$.  Below we will discuss how one constructs this distribution, but lets take it as given for the time being.  Once one has the distribution, then one can calculate the $p$-value is given by
\begin{equation}
p(\vec\alpha) = \int_{T_0}^\infty f(T | \vec\alpha) dT = \int  \f(\data | \vec\alpha )\, \theta(T(\data) - T_0) \,d\data = P(T\ge T_0 | \vec\alpha) \;,
\end{equation}
where $T_0$ is the value of the test statistic based on the observed data and $\theta( \cdot )$ is the Heaviside function.\footnote{The integral $\int d\data$ is a bit unusual for a marked Poisson model, because it involves both a sum over the number of events and an integral over the values of $x_e$ for each of those events.} Usually the $p$-value is just written as $p$, but I have written it as $p(\vec\alpha)$ to make its  $\vec\alpha$-dependence explicit.  


Given that the $p$-value depends on $\vec\alpha$, how does one decide to accept or reject the null hypothesis?  Remembering that $\vec\alpha_{\rm poi}$ takes on a specific value for the null hypothesis, we are worried about how the $p$-value changes as a function of the nuisance parameters.  It is natural to say that one should not reject the null hypothesis if the $p$-value is larger than the size of the test \textit{for any value of the nuisance parameters}.  Thus, in a frequentist approach one should either present $p$-value explicitly as a function of $\vec{\alpha}_{\rm nuis}$ or take its maximal (or supremum) value 
\begin{equation}
p_{\rm sup}(\vec\alpha_{\rm poi}) = \sup_{ \vec{\alpha}_{\rm nuis}} p(\vec{\alpha}_{\rm nuis}) \; .
\end{equation}


As a final note it is worth mentioning that the size of the test, which serves as the threshold for rejecting the null hypothesis, is purely conventional.  In most sciences conventional choices of the size are 10\%, 5\%, or 1\%.  In particle physics, our conventional threshold for discovery is the infamous $5\sigma$ criterion -- which is a conventional way to refer to $\alpha=2.87 \cdot 10^{-7}$.  This is an incredibly small rate of Type-I error, reflecting that claiming the discovery of new physics would be a monumental statement.  The origin of the $5\sigma$ criterion has its roots in the fact that traditionally we lacked the tools to properly incorporate systematics, we fear that there are systematics that may not be fully under control, and we perform many searches for new physics and thus we have many chances to reject the background-only hypothesis.  We will return to this in the discussion of the look-elsewhere effect. 


\subsection{Excluded and allowed regions as confidence intervals}


Often we consider a new physics model that is parametrized by theoretical parameters.  For instance, the mass or coupling of a new particle.  In that case we typically want to ask what values of these theoretical parameters are allowed or excluded given available data.  Figure~\ref{fig:confidenceIntervals} shows two examples.  Figure~\ref{fig:confidenceIntervals}(a) shows an example with $\vec\alpha_{\rm poi} = (\sigma/\sigma_{SM}, M_H)$, where $\sigma/\sigma_{SM}$ is the ratio of the production cross-section for the Higgs boson with respect to its prediction in the standard model and $M_H$ is the unknown Higgs mass parameter in the standard model.  All the parameter points above the solid black curve correspond to scenarios for the Higgs boson that are considered `excluded at the 95\% confidence level'.  Figure~\ref{fig:confidenceIntervals}(b) shows an example with 
$\vec\alpha_{\rm poi} = (m_W,m_t)$ where $m_W$ is the mass of the $W$-boson and $m_t$ is the mass of the top quark. We have discovered the $W$-boson and the top quark and measured their masses.  The blue ellipse `is the 68\% confidence level contour' and all the parameter points inside it are considered `consistent with data at the $1\sigma$ level'.   What is the precise meaning of these statements?
