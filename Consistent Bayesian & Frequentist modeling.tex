\subsubsection{Consistent Bayesian and Frequentist modeling}
\label{S:ConstraintExamples} 

The variational estimates $\eta^\pm$ and $\sigma^\pm$ typically correspond to so called ``$\pm 1\sigma$ variations'' in the source of the uncertainty.  Here we are focusing on the source of the uncertainty, not its affect on rates and shapes.  For instance, we might say that the jet energy scale has a 10\% uncertainty.~\footnote{Without loss of generality, we choose to parametrize $\alpha_p$ such that $\alpha_p=0$ is the nominal value of this parameter, $\alpha_p=\pm 1$ are the ``$\pm 1\sigma$ variations''.}  This is common jargon, but what does it mean?  The most common interpretation of this statement is that the uncertain parameter $\alpha_p$ (eg. the jet energy scale) has a Gaussian distribution.  However, this way of thinking is manifestly Bayesian.  If the parameter was estimated from an auxiliary measurement, then it is the PDF for that measurement that we wish to include into our probability model.  In the frequentist way of thinking, the jet energy scale has an unknown true value and upon repeating the experiment many times the auxiliary measurements estimating the jet energy scale would fluctuate randomly about this true value.  To aid in this subtle distinction, we use greek letters for the parameters (eg. $\alpha_p$) and roman letters for the auxiliary measurements $a_p$.  Furthermore, we interpret the ``$\pm 1\sigma$'' variation in the frequentist sense, which leads to the constraint term $f_p(a_p | \alpha_p)$.  Then, we can pair the resulting likelihood with some prior on $\alpha_p$ to form a Bayesian posterior if we wish according to Eq.~\ref{eq:urprior}.


It is often advocated that a ``log-normal'' or ``gamma'' distribution for $\alpha_p$ is more appropriate than a gaussian constraint~\cite{CousinsLogNormal}.  This is particularly clear in the case of bounded parameters and large uncertainties.    Here we must take some care to build a probability model that can maintain a consistent interpretation in Bayesian a frequentist settings.  Table~\ref{tab:constraints} summarizes a few consistent treatments of the frequentist pdf, the likelihood function, a prior, and the resulting posterior.


\begin{table}
\center
\begin{tabular}{llll}
PDF & Likelihood $\propto$ & Prior $\pi_0$ & Posterior $\pi$ \\ \hline
$G(a_p | \alpha_p, \sigma_p)$ & $G(\alpha_p | a_p, \sigma_p)$ & $\pi_0(\alpha_p)\propto$  const & $G(\alpha_p | a_p, \sigma_p)$ \\
$\Pois(n_p | \tau_p \beta_p)$ & $\PGamma(\beta_p | A=\tau_p; B=1+n_p)$ & $\pi_0(\beta_p) \propto$  const & $\PGamma(\beta_p | A=\tau_p; B=1+n_p)$ \\
$\LN(n_p | \beta_p, \sigma_p)$ & $ \beta_p  \cdot \LN(\beta_p | n_p, \sigma_p)$ & $\pi_0(\beta_p) \propto $ const & $\LN(\beta_p | n_p, \sigma_p)$ \\
$\LN(n_p | \beta_p, \sigma_p)$ & $\beta_p  \cdot\LN(\beta_p | n_p, \sigma_p)$ & $\pi_0(\beta_p) \propto 1/\beta_p $  & $\LN(\beta_p | n_p, \sigma_p)$\\
%$G(\ln(a_p) | \ln(\alpha_p), \ln(\sigma_p))$ & $\LN(G(\ln(\alpha_p) | a_p, \sigma_p) = $ & $\pi_0(\alpha_p)\propto$  const & $G(\alpha_p | a_p, \sigma_p)$ \\
\end{tabular}
\caption{Table relating consistent treatments of PDF, likelihood, prior, and posterior for nuisance parameter constraint terms.}
\label{tab:constraints}
\end{table}


Finally, it is worth mentioning that the uncertainty on some parameters is not the result of an auxiliary measurement -- so the constraint term idealization, it is not just a convenience, but a real  conceptual leap.  This is particularly true for theoretical uncertainties from higher-order corrections or renormalizaiton and factorization scale dependence.  In these cases a formal frequentist analysis would not include a constraint term for these parameters, and the result would simply depend on their assumed values.  As this is not the norm, we can think of reading Table~\ref{tab:constraints} from right-to-left with a subjective Bayesian prior $\pi(\alpha)$ being interpreted as coming from a fictional auxiliary measurement.


The Gaussian constraint for $\alpha_p$ corresponds to the familiar situation.  It is a good approximation of the auxiliary measurement when the likelihood function for $\alpha_p$ from that auxiliary measurement has a Gaussian shape.  More formally, it is valid when the maximum likelihood estimate of $\alpha_p$ (eg. the best fit value of $\alpha_p$) has a Gaussian distribution.  Here we can identify the maximum likelihood estimate of $\alpha_p$ with the global observable $a_p$, remembering that it is a number that is extracted from the data and thus its distribution has a frequentist interpretation.  
\begin{equation}
G(a_p | \alpha_p, \sigma_p) = \frac{1}{\sqrt{2\pi \sigma_p^2}} \exp \left[ -\frac{(a_p - \alpha_p)^2}{2\sigma_p^2} \right]
\end{equation}
with $\sigma_p=1$ by default.
Note that the PDF of $a_p$ and the likelihood for $\alpha_p$ are positive for all values. 


When the auxiliary measurement is actually based on counting events in a control region (eg. a Poisson process), a more accurate to describe the auxiliary measurement with a Poisson distribution.  It has been shown that the truncated Gaussian constraint can lead to undercoverage (overly optimistic) results, which makes this issue practically relevant~\cite{Cousins:2008zz}.  Table~\ref{tab:constraints} shows that a Poisson PDF together with a uniform prior leads to a gamma posterior, thus this type of constraint is often called a ``gamma'' constraint.  This is a bit unfortunate since the gamma distribution is manifestly Bayesian and with a different choice of prior, one might not arrive at a gamma posterior.  When dealing with the Poisson constraint, it is no longer convenient to work with  our conventional scaling for $\alpha_p$ which can be negative.  Instead, it is more natural to think of the number of events measured in the auxiliary measurement $n_p$ and the mean of the Poisson parameter.  This information is not usually available, instead one usually has some notion of the relative uncertainty in the parameter $\sigma_p^{\rm rel}$ (eg. a the jet energy scale is known to 10\%).  In order to give some uniformity to the different uncertainties of this type and think of relative uncertainty, the nominal rate is factored out into a constant $\tau_p$ and the mean of the Poisson is given by $\tau_p \alpha_p$.  
\begin{equation}
\Pois(n_p | \tau_p \alpha_p) =\frac{ (\tau_p \alpha_p)^{n_p} \; e^{-\tau_p \alpha_p} } {n_p!}
\end{equation}
Here we can use the fact that Var$[n_p]=\sqrt{\tau_p\alpha_p}$ and reverse engineer the nominal auxiliary measurement 
\begin{equation}
n_p^0 =  \tau_p = (1/\sigma_{p}^{\rm rel})^2\; .
\end{equation}
where the superscript $0$ is to remind us that $n_p$ will fluctuate in repeated experiments but $n_p^0$ is the value of our measured estimate of the parameter.


One important thing to keep in mind is that there is only one constraint term per nuisance parameter, so there must be only one $\sigma_p^{rel}$ per nuisance parameter.  This $\sigma_p^{rel}$ is related to the fundamental uncertainty in the source and we cannot infer this from the various response terms $\eta_{ps}^\pm$ or $\sigma_{pub}^\pm$. 


Another technical difficulty is that the Poisson distribution is discrete. So if one were to say the relative uncertainty was 30\%, then we would find $n_p^0=11.11...$, which is not an integer.  Rounding $n_p$ to the nearest integer while maintaining $\tau_p= (1/\sigma_{p}^{\rm rel})^2$ will bias the maximum likelihood estimate of $\alpha_p$ away from 1.  To avoid this, one can use the gamma distribution, which generalizes more continuously with 
\begin{equation}
\PGamma(\alpha_p | A=\tau_p, B=n_p-1) = A (A \alpha_p)^{B} e^{-A \alpha_p} / \Gamma(B)\;.
\end{equation}
This approach works fine for likelihood fits, Bayesian calculations, and frequentist techniques based on asymptotic approximations, but it does not offer a consistent treatment of the pdf for the global observable $n_p$ that is needed for techniques based on Monte Carlo sampling. 


From Eadie et al., ``The log-normal distribution represents a random variable whose logarithm follows a normal distribution. It provides a model for the error of a process involving many small multiplicative errors (from the Central Limit Theorem). It is also appropriate when the value of an observed variable is a random proportion of the previous observation.''~\cite{Eadie:qy,CousinsLogNormal}.  This logic of multiplicative errors applies to the the measured value, not the parameter.  Thus, it is natural to say that there is some auxiliary measurement (global observable) with a log-normal distribution.  As in the gamma/Poisson case above, let us again say that the global observable is $n_p$ with a nominal value
\begin{equation}
n_p^0 =  \tau_p = (1/\sigma_{p}^{\rm rel})^2\; .
\end{equation}
Then the conventional choice for the corresponding log-normal distribution is
\begin{equation}
\LN(n_p | \alpha_p, \kappa_p) = \frac{1}{\sqrt{2\pi}\ln \kappa}\frac{1}{n_p} \exp \left[ -\frac{\ln(n_p/ \alpha_p)^2}{2(\ln \kappa_p)^2} \right]
\end{equation}
while the likelihood function is (blue curve in Fig.~\ref{fig:lognormal}(a)).
\begin{equation}
L( \alpha_p) = \frac{1}{\sqrt{2\pi}\ln \kappa}\frac{1}{n_p} \exp \left[ -\frac{\ln(n_p/ \alpha_p)^2}{2(\ln \kappa_p)^2} \right];.
\end{equation}
To get to the posterior for $\alpha_p$ given $n_p$ we need an ur-prior $\eta(\alpha_p$)
\begin{equation}
\pi( \alpha_p) \propto \eta(\alpha_p)  \; \frac{1}{\sqrt{2\pi}\ln \kappa}\frac{1}{n_p} \exp \left[ -\frac{\ln(n_p/ \alpha_p)^2}{2(\ln \kappa_p)^2} \right]
\end{equation}
If $\eta(\alpha_p)$ is uniform, then the posterior looks like the red curve in Fig.~\ref{fig:lognormal}(b).  However, when paired with an ``ur-prior'' $\eta(\alpha_p) \propto 1/\alpha_p$ (green curve in Fig.~\ref{fig:lognormal}(b)), this results in a posterior distribution that is also of a log-normal form for $\alpha_p$ (blue curve in Fig.~\ref{fig:lognormal}(b)).
